{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c1719e-bc02-43c9-befa-3431c7631eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchtext==0.4.0\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d74f13-5237-493b-ad03-db4c7520b0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install \"numpy<2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c89339-48a4-433b-a8fd-20f2bb87d9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data, datasets\n",
    "import random\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# TEXT field for tokenization\n",
    "TEXT = data.Field(tokenize='spacy', \n",
    "                  tokenizer_language='en_core_web_sm', \n",
    "                  include_lengths=True)\n",
    "\n",
    "# LABEL field for the multi-class classification labels \n",
    "LABEL = data.LabelField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5ec84b-3f85-4e3e-9246-b5317949d81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split dataaset into the predefined training and test sets \n",
    "train_data, test_data = datasets.TREC.splits(TEXT, LABEL, fine_grained=False)\n",
    "\n",
    "# Split training data into train and validation sets \n",
    "train_data, valid_data = train_data.split(split_ratio=0.8, \n",
    "                                          stratified=True, \n",
    "                                          strata_field='label', \n",
    "                                          random_state=random.seed(SEED))\n",
    "# Check data\n",
    "print(f\"Number of training examples: {len(train_data)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data)}\")\n",
    "print(f\"Number of testing examples: {len(test_data)}\")\n",
    "\n",
    "# Print example\n",
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c335f8c6-aff8-49c5-9f0e-d14d22a90b89",
   "metadata": {},
   "source": [
    "Part A "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a892093-c0cb-4172-9a29-e8cb0c223eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose pre_trained_vectors\n",
    "PRE_TRAINED_VECTORS = \"glove.6B.300d\" \n",
    "\n",
    "# Build vocabulary\n",
    "TEXT.build_vocab(train_data, \n",
    "                 vectors=PRE_TRAINED_VECTORS, \n",
    "                 unk_init=torch.Tensor.normal_) \n",
    "#Q1(c) unk_init=torch.Tensor.normal_ initialized all words not found in GloVe (our OOV words) with random numbers from a normal distribution\n",
    "\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3d0ba0-be8c-47af-b1cf-813963a77820",
   "metadata": {},
   "source": [
    "Q1(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e9c9dd-19b5-46a3-938e-91da7a9e7b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary size\n",
    "vocab_size = len(TEXT.vocab)\n",
    "print(f\"The size of the vocabulary from the training data is: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b821af45-5a1f-4dda-b7d6-cccec09102c1",
   "metadata": {},
   "source": [
    "Q1(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d1906b-2a10-4561-897a-14c36d04c410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1(b)\n",
    "\n",
    "# Load the pre-trained vector dictionary manually to check for existence\n",
    "from torchtext.vocab import GloVe\n",
    "glove_vectors = GloVe(name='6B', dim=300)\n",
    "\n",
    "# Count total OOV words\n",
    "oov_words = []\n",
    "for word in TEXT.vocab.itos:\n",
    "    if word not in glove_vectors.stoi: \n",
    "        if word not in ['<unk>', '<pad>']:\n",
    "            oov_words.append(word)\n",
    "\n",
    "print(f\"Total number of OOV words: {len(oov_words)}\")\n",
    "\n",
    "# Number of unique OOV words per topic category\n",
    "import collections\n",
    "oov_by_topic = collections.defaultdict(set)\n",
    "for example in train_data.examples:\n",
    "    topic = example.label\n",
    "    for word in example.text:\n",
    "        if word in oov_words:\n",
    "            oov_by_topic[topic].add(word)\n",
    "\n",
    "print(\"\\nNumber of OOV words per topic category:\")\n",
    "for topic, words in oov_by_topic.items():\n",
    "    print(f\"{topic}: {len(words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54faa59d-fc41-4ee7-9033-aa83dac219e4",
   "metadata": {},
   "source": [
    "Q1(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0ef04f-7963-4eea-901e-e65a7cd05bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import collections\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np  \n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "topic_word_counts = collections.defaultdict(collections.Counter)\n",
    "for ex in train_data.examples:\n",
    "    topic = ex.label\n",
    "    for word in ex.text:\n",
    "        if word not in stop_words and word.isalpha(): # Check if alphabetic\n",
    "            topic_word_counts[topic][word] += 1\n",
    "\n",
    "top_words_data = []\n",
    "for topic, counter in topic_word_counts.items():\n",
    "    words_added = 0 \n",
    "    for word, freq in counter.most_common(100): \n",
    "        \n",
    "        if words_added >= 20:\n",
    "            break\n",
    "            \n",
    "        if word in TEXT.vocab.stoi and word not in stop_words and word.isalpha():\n",
    "            if word not in oov_words: \n",
    "                vector = TEXT.vocab.vectors[TEXT.vocab.stoi[word]].numpy()\n",
    "                top_words_data.append({'word': word, 'topic': topic, 'vector': vector})\n",
    "                words_added += 1 \n",
    "\n",
    "df = pd.DataFrame(top_words_data)\n",
    "\n",
    "\n",
    "vectors_for_tsne = np.array(list(df['vector']))\n",
    "\n",
    "vectors_2d = TSNE(n_components=2, random_state=SEED).fit_transform(vectors_for_tsne)\n",
    "\n",
    "df['x'] = vectors_2d[:, 0]\n",
    "df['y'] = vectors_2d[:, 1]\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.scatterplot(data=df, x='x', y='y', hue='topic', legend='full', s=100) # s=size\n",
    "\n",
    "# Add labels\n",
    "for i, row in df.iterrows():\n",
    "    plt.text(row['x'] + 0.1, row['y'], row['word'], fontsize=9)\n",
    "\n",
    "plt.title('t-SNE Projection of Top 20 Words per Topic')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b082b7a-f8ed-4395-b821-2ca19374feac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
