{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1debcd3",
   "metadata": {},
   "source": [
    "# SC4002 Assignment - Part 3.1, 3.3, & 3.4: Hyperparameter Tuning\n",
    "\n",
    "**Team 3: Aaron Chen & Javier Tin**\n",
    "\n",
    "This notebook performs a **reproducible grid search** to find the optimal hyperparameters for all required models:\n",
    "1.  **BiLSTM (Part 3.1)**\n",
    "2.  **BiGRU (Part 3.1)**\n",
    "3.  **BiLSTM + Attention (Part 3.3)**\n",
    "4.  **BiLSTM + Focal Loss (Part 3.4)**\n",
    "\n",
    "This will allow a fair comparison, as we will find the best-performing version of each architecture.\n",
    "\n",
    "All training is **seeded (SEED = 42)** for reproducible results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cce7ee6",
   "metadata": {},
   "source": [
    "## 1. Imports & Setup\n",
    "\n",
    "This cell imports all data from our compliant `data_pipeline.py` and sets the random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eb5a3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\nlp_project\\Lib\\site-packages\\torchtext\\vocab.py:432: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.itos, self.stoi, self.vectors, self.dim = torch.load(path_pt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Successfully imported data pipeline.\n",
      "  - Using device: cuda\n",
      "  - Batch Size: 64\n"
     ]
    }
   ],
   "source": [
    "# === Core PyTorch Imports ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import itertools\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# === Plotting Imports ===\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Sets the random seed for full reproducibility.\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.use_deterministic_algorithms(True) \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False \n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8' \n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "# === Check PyTorch and CUDA Versions ===\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# === Data Pipeline Import ===\n",
    "try:\n",
    "    from data_pipeline import (\n",
    "        train_iterator, \n",
    "        valid_iterator, \n",
    "        test_iterator, \n",
    "        TEXT, \n",
    "        LABEL, \n",
    "        create_embedding_layer,\n",
    "        device,\n",
    "        BATCH_SIZE\n",
    "    )\n",
    "    print(\"\\n✓ Successfully imported data pipeline.\")\n",
    "    print(f\"  - Using device: {device}\")\n",
    "    print(f\"  - Batch Size: {BATCH_SIZE}\")\n",
    "except ImportError:\n",
    "    print(\"--- ERROR ---\")\n",
    "    print(\"Could not find 'data_pipeline.py'.\")\n",
    "    print(\"Please make sure 'data_pipeline.py' is in the same directory as this notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68f13e2",
   "metadata": {},
   "source": [
    "## 2. Define Hyperparameter Grid\n",
    "\n",
    "We will test all four models (BiLSTM, BiGRU, Attention, FocalLoss) across a range of hyperparameters.\n",
    "\n",
    "**Note:** `Attention` refers to the `ImprovedBiLSTM_Attention` architecture.\n",
    "**Note:** `FocalLoss` refers to the *baseline `BiLSTM_Model`* architecture, but trained with `FocalLoss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aaeb200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total combinations to test: 162\n",
      "\n",
      "--- First 5 Combinations ---\n",
      "{'model_type': 'Attention', 'hidden_dim': 256, 'n_layers': 2, 'dropout': 0.4, 'weight_decay': 5e-06}\n",
      "{'model_type': 'Attention', 'hidden_dim': 256, 'n_layers': 2, 'dropout': 0.4, 'weight_decay': 1e-05}\n",
      "{'model_type': 'Attention', 'hidden_dim': 256, 'n_layers': 2, 'dropout': 0.4, 'weight_decay': 5e-05}\n",
      "{'model_type': 'Attention', 'hidden_dim': 256, 'n_layers': 2, 'dropout': 0.5, 'weight_decay': 5e-06}\n",
      "{'model_type': 'Attention', 'hidden_dim': 256, 'n_layers': 2, 'dropout': 0.5, 'weight_decay': 1e-05}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'model_type': ['Attention', 'FocalLoss'],\n",
    "    'hidden_dim': [256, 384, 512],\n",
    "    'n_layers': [2, 3, 4],\n",
    "    'dropout': [0.4, 0.5, 0.6],\n",
    "    'weight_decay': [5e-6, 1e-5, 5e-5] \n",
    "}\n",
    "\n",
    "# Create all combinations\n",
    "keys, values = zip(*param_grid.items())\n",
    "hyperparam_combos = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "print(f\"Total combinations to test: {len(hyperparam_combos)}\")\n",
    "print(\"\\n--- First 5 Combinations ---\")\n",
    "for combo in hyperparam_combos[:5]:\n",
    "    print(combo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9716c032",
   "metadata": {},
   "source": [
    "## 3. Model & Training Definitions\n",
    "\n",
    "Here we define all models (Baselines and Improved) and the Focal Loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef2fa90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All model and loss function classes defined.\n"
     ]
    }
   ],
   "source": [
    "# === 1. Baseline BiLSTM Model (Part 3.1) ===\n",
    "class BiLSTM_Model(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = create_embedding_layer(freeze=False)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=emb_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout if n_layers > 1 else 0,\n",
    "            batch_first=False\n",
    "        )\n",
    "        fc_input_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.fc = nn.Linear(fc_input_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, text, lengths):\n",
    "        embedded = self.embedding(text)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, lengths.to('cpu'), enforce_sorted=False)\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        last_hidden_state = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        dropped_hidden = self.dropout(last_hidden_state)\n",
    "        prediction = self.fc(dropped_hidden)\n",
    "        return prediction\n",
    "\n",
    "# === 2. Baseline BiGRU Model (Part 3.1) ===\n",
    "class BiGRU_Model(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = create_embedding_layer(freeze=False)\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=emb_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout if n_layers > 1 else 0,\n",
    "            batch_first=False\n",
    "        )\n",
    "        fc_input_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.fc = nn.Linear(fc_input_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, text, lengths):\n",
    "        embedded = self.embedding(text)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, lengths.to('cpu'), enforce_sorted=False)\n",
    "        packed_output, hidden = self.gru(packed_embedded)\n",
    "        last_hidden_state = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        dropped_hidden = self.dropout(last_hidden_state)\n",
    "        prediction = self.fc(dropped_hidden)\n",
    "        return prediction\n",
    "\n",
    "# === 3. IMPROVED Attention Module (Part 3.3) ===\n",
    "class ImprovedAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.context_vector = nn.Linear(hidden_dim * 2, 1, bias=False)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, lstm_output, final_hidden):\n",
    "        lstm_output = lstm_output.permute(1, 0, 2)\n",
    "        energy = torch.tanh(self.attention(lstm_output))\n",
    "        attention_weights = F.softmax(self.context_vector(energy), dim=1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        context = torch.bmm(attention_weights.transpose(1, 2), lstm_output)\n",
    "        return context.squeeze(1)\n",
    "\n",
    "# === 4. IMPROVED BiLSTM + Attention (Part 3.3) ===\n",
    "class ImprovedBiLSTM_Attention(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = create_embedding_layer(freeze=False)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=emb_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout if n_layers > 1 else 0,\n",
    "            batch_first=False\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim * 2)\n",
    "        self.attention = ImprovedAttention(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, lengths):\n",
    "        embedded = self.embedding(text)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, lengths.to('cpu'), enforce_sorted=False)\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=False)\n",
    "        output = self.layer_norm(output)\n",
    "        context_vector = self.attention(output, hidden)\n",
    "        dropped_context = self.dropout(context_vector)\n",
    "        prediction = self.fc(dropped_context)\n",
    "        return prediction\n",
    "\n",
    "# === 5. IMPROVED Focal Loss (Part 3.4) ===\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, label_smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.label_smoothing = label_smoothing\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', label_smoothing=self.label_smoothing)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "# === Training/Evaluation Function Definitions ===\n",
    "\n",
    "def get_accuracy(preds, y):\n",
    "    top_pred = preds.argmax(1, keepdim=True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc\n",
    "\n",
    "def train_epoch(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text, lengths = batch.text\n",
    "        predictions = model(text, lengths)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        loss.backward()\n",
    "        # Add gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "def evaluate_epoch(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, lengths = batch.text\n",
    "            predictions = model(text, lengths)\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = get_accuracy(predictions, batch.label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "print(\"✓ All model and loss function classes defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47541b39",
   "metadata": {},
   "source": [
    "## 4. Run Grid Search\n",
    "\n",
    "This will loop through all combinations. This cell will take a long time to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efacefe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Grid Search for 162 combinations --- This will take some time. ---\n",
      "\n",
      "==================== RUN 1/162 ====================\n",
      "Params: {'model_type': 'Attention', 'hidden_dim': 256, 'n_layers': 2, 'dropout': 0.4, 'weight_decay': 5e-06}\n"
     ]
    }
   ],
   "source": [
    "# === Get Static Parameters ===\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "OUTPUT_DIM = len(LABEL.vocab)\n",
    "EMBEDDING_DIM = create_embedding_layer().embedding_dim\n",
    "N_EPOCHS = 10 # Train each combo for 10 epochs\n",
    "\n",
    "grid_search_results = []\n",
    "total_runs = len(hyperparam_combos)\n",
    "\n",
    "print(f\"--- Starting Grid Search for {total_runs} combinations --- This will take some time. ---\")\n",
    "\n",
    "for i, params in enumerate(hyperparam_combos):\n",
    "    run_num = i + 1\n",
    "    print(f\"\\n{'='*20} RUN {run_num}/{total_runs} {'='*20}\")\n",
    "    print(f\"Params: {params}\")\n",
    "    \n",
    "    # 1. Instantiate Model based on model_type\n",
    "    model_type = params['model_type']\n",
    "    \n",
    "    if model_type == 'BiLSTM':\n",
    "        model = BiLSTM_Model(\n",
    "            INPUT_DIM, EMBEDDING_DIM, params['hidden_dim'], OUTPUT_DIM,\n",
    "            params['n_layers'], True, params['dropout']\n",
    "        ).to(device)\n",
    "    elif model_type == 'BiGRU':\n",
    "        model = BiGRU_Model(\n",
    "            INPUT_DIM, EMBEDDING_DIM, params['hidden_dim'], OUTPUT_DIM,\n",
    "            params['n_layers'], True, params['dropout']\n",
    "        ).to(device)\n",
    "    elif model_type == 'Attention':\n",
    "        model = ImprovedBiLSTM_Attention(\n",
    "            INPUT_DIM, EMBEDDING_DIM, params['hidden_dim'], OUTPUT_DIM,\n",
    "            params['n_layers'], True, params['dropout']\n",
    "        ).to(device)\n",
    "    elif model_type == 'FocalLoss':\n",
    "        model = BiLSTM_Model( # Using baseline arch for this test\n",
    "            INPUT_DIM, EMBEDDING_DIM, params['hidden_dim'], OUTPUT_DIM,\n",
    "            params['n_layers'], True, params['dropout']\n",
    "        ).to(device)\n",
    "    \n",
    "    # 2. Instantiate Optimizer and Criterion\n",
    "    optimizer = optim.Adam(model.parameters(), weight_decay=params['weight_decay'])\n",
    "    \n",
    "    if model_type == 'FocalLoss':\n",
    "        criterion = FocalLoss(alpha=1, gamma=2, label_smoothing=0.1).to(device)\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    best_valid_acc = -1.0\n",
    "    start_run_time = time.time()\n",
    "    \n",
    "    # 3. Training Loop for this combination\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        train_epoch(model, train_iterator, optimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate_epoch(model, valid_iterator, criterion)\n",
    "        \n",
    "        if valid_acc > best_valid_acc:\n",
    "            best_valid_acc = valid_acc\n",
    "    \n",
    "    end_run_time = time.time()\n",
    "    run_duration_mins = (end_run_time - start_run_time) / 60\n",
    "    \n",
    "    # 4. Store Results\n",
    "    result = params.copy()\n",
    "    result['best_valid_acc'] = best_valid_acc * 100 # As percentage\n",
    "    result['time_mins'] = run_duration_mins\n",
    "    grid_search_results.append(result)\n",
    "    \n",
    "    print(f\"Run {run_num} complete. Time: {run_duration_mins:.2f}m. Best Valid Acc: {best_valid_acc*100:.2f}%\")\n",
    "\n",
    "print(\"\\n--- GRID SEARCH COMPLETE ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8a1596",
   "metadata": {},
   "source": [
    "## 5. Analyze Results\n",
    "\n",
    "Now we can load all the new results into a `pandas.DataFrame` to find the winning model for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd4846f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved all tuning results to grid_search_results_all.json\n",
      "\n",
      "======================================================\n",
      "           WINNING HYPERPARAMETERS\n",
      "======================================================\n",
      "\n",
      "--- [Part 3.3] Best BiLSTM + Attention ---\n",
      "Accuracy: 87.24%\n",
      "{'model_type': 'Attention', 'hidden_dim': 384, 'n_layers': 4, 'dropout': 0.5, 'weight_decay': 5e-06, 'time_mins': 0.25966018438339233}\n",
      "\n",
      "--- [Part 3.4] Best BiLSTM + Focal Loss ---\n",
      "Accuracy: 88.80%\n",
      "{'model_type': 'FocalLoss', 'hidden_dim': 384, 'n_layers': 3, 'dropout': 0.6, 'weight_decay': 1e-05, 'time_mins': 0.18910448948542277}\n",
      "\n",
      "======================================================\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(grid_search_results)\n",
    "\n",
    "# Save all results to one file\n",
    "results_df.to_json(\"grid_search_results_all.json\", orient=\"records\", indent=4)\n",
    "print(\"✓ Saved all tuning results to grid_search_results_all.json\")\n",
    "\n",
    "# --- Find the best model for each category ---\n",
    "print(\"\\n======================================================\")\n",
    "print(\"           WINNING HYPERPARAMETERS\")\n",
    "print(\"======================================================\")\n",
    "\n",
    "\n",
    "\n",
    "# 3.3 - Best Attention Model\n",
    "best_attention = results_df[results_df['model_type'] == 'Attention'].sort_values(by='best_valid_acc', ascending=False).iloc[0]\n",
    "print(\"\\n--- [Part 3.3] Best BiLSTM + Attention ---\")\n",
    "print(f\"Accuracy: {best_attention['best_valid_acc']:.2f}%\")\n",
    "print(best_attention.drop('best_valid_acc').to_dict())\n",
    "\n",
    "# 3.4 - Best Focal Loss Model\n",
    "best_focal = results_df[results_df['model_type'] == 'FocalLoss'].sort_values(by='best_valid_acc', ascending=False).iloc[0]\n",
    "print(\"\\n--- [Part 3.4] Best BiLSTM + Focal Loss ---\")\n",
    "print(f\"Accuracy: {best_focal['best_valid_acc']:.2f}%\")\n",
    "print(best_focal.drop('best_valid_acc').to_dict())\n",
    "\n",
    "print(\"\\n======================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770dced4",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "1.  This notebook has found the **best** hyperparameters for all four models.\n",
    "2.  Now, create a **new, clean notebook** (like your `SC4002_RNN_Experiments.ipynb`).\n",
    "3.  In that notebook, train **only these 4 winning models** using their optimized parameters.\n",
    "4.  Run `get_topic_accuracy` on all of them and generate your final comparison tables and conclusions for the report."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
